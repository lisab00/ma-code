{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8176472e",
   "metadata": {},
   "source": [
    "# Inverse UQ of 'a' and 'm' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b5b25a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `C:\\Users\\lisah\\Documents\\Repos\\ma-code`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"C:/Users/lisah/Documents/Repos/ma-code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c5c793f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Src.\n",
      "WARNING: using Src.gen_all_fish_data in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "include(\"c:/Users/lisah/Documents/Repos/ma-code/src/src.jl\")\n",
    "using .Src, DataFrames, Optim, ForwardDiff, LinearAlgebra, CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42037046",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf328d9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mult_restart_mle_am"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    function compute_ll_am(x, hprm::Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "compute the log-likelihood in least-squares form for Klausmeier model for data with Gaussian noise. First, simulate Klausmeier model for given hyperparameters and noise level. Then, compare to true trajectories.\n",
    "x contains the parameters of interest, which should be inferred.\n",
    "\n",
    "# Arguments\n",
    "- `x`: variables with respect to which likelihood is computed\n",
    "- `hprm::Hyperprm`: parameters for which the Klausmeier simulation is performed\n",
    "- `true_val::DataFrame`: observed data trajectories. DataFrame with columns \"w\" and \"n\".\n",
    "- `t_fixed::Bool`: true if we consider a fixed observation time window\n",
    "- `t_end::Float64`: end of observation window (if t_fixed=true)\n",
    "- `t_step::Float64`: step size with which M observations should be picked (set if t_fixed=false)\n",
    "\n",
    "# Returns\n",
    "- `Float`: scalar value of log-likelihood at given grid point \n",
    "\"\"\"\n",
    "function compute_ll_am(x, hprm::Src.Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "    a, m = x\n",
    "    hprm = Src.Hyperprm(hprm.w0, hprm.n0, a, m, hprm.M, hprm.noise)\n",
    "    pred_val = Src.sol_klausmeier(hprm; t_fixed=t_fixed, t_end=t_end,t_step=t_step)\n",
    "    if hprm.noise == 0.0\n",
    "        ll = -0.5 * sum((true_val[:,\"n\"] - pred_val[:,\"n\"]) .^2) - 0.5 * sum((true_val[:,\"w\"] - pred_val[:,\"w\"]) .^2) # add up ll for both trajectories\n",
    "    else\n",
    "        ll = -0.5 * 1/hprm.noise * sum((true_val[:,\"n\"] - pred_val[:,\"n\"]) .^2) - 0.5 * 1/hprm.noise * sum((true_val[:,\"w\"] - pred_val[:,\"w\"]) .^2) # add up ll for both trajectories\n",
    "    end\n",
    "    return ll\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    function compute_mle(hprm::Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "compute the maximum likelihood estimate given data observations by minimizing the negative log-likelihood function using the Optim.jl package.\n",
    "The initialization point is chosen as the true parameter combination underlying the data observation to ensure fast convergence to global minimum.\n",
    "The minimization method is chosen by default.\n",
    "\n",
    "# Returns\n",
    "- `Vector{Float64}`: 2-element vector containing the mle [a_mle, n0_mle]\n",
    "- `Bool`: true if optimization was successfull\n",
    "\"\"\"\n",
    "function compute_mle_am(hprm::Src.Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0, N::Int64=5)\n",
    "    inits, inits_loss, mles, losses, best_loss_ind, converged = mult_restart_mle_am(N, hprm, true_val; t_fixed=t_fixed, t_end=t_end, t_step=t_step)\n",
    "    return mles[best_loss_ind, :], converged[best_loss_ind]\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    function mult_restart_mle(N::Int64, hprm::Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "Perform Maximum Likelihood estimation for N different starting points. Goal is to find global minimum\n",
    "\n",
    "# Inputs\n",
    "    - `N::Int64`: number of restarts\n",
    "\n",
    "# Returns\n",
    "    - `Matrix`: initial values used in optimization\n",
    "    - `Vector`: losses of initial values\n",
    "    - `Matrix`: computed MLEs\n",
    "    - `Vector`: corresponding losses of MLEs\n",
    "    - `Int`: index of optimization trial creating minimal loss\n",
    "    - `Vector`: convergence status for each optimization trial\n",
    "\"\"\"\n",
    "function mult_restart_mle_am(N::Int64, hprm::Src.Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "    # generate optim start pts\n",
    "    inits = hcat(2 .* rand(N), 4 .* rand(N))\n",
    "\n",
    "    # store mles and corresponding loss\n",
    "    mle_vals = zeros(N, 2)\n",
    "    mle_loss, inits_loss, converged = zeros(N), zeros(N), zeros(N)\n",
    "\n",
    "    for i in 1:N\n",
    "        pt = inits[i,:] # starting point for optimization\n",
    "        result = optimize(x -> - compute_ll_am(x, hprm, true_val; t_fixed=t_fixed, t_end=t_end, t_step=t_step), pt)\n",
    "        #display(result)\n",
    "        mle_vals[i,:] = Optim.minimizer(result)\n",
    "        mle_loss[i] =  Optim.minimum(result)\n",
    "        converged[i] = Optim.converged(result)\n",
    "        inits_loss[i] = -compute_ll_am(pt, hprm, true_val; t_fixed=t_fixed, t_end=t_end, t_step=t_step)\n",
    "    end\n",
    "\n",
    "    # extract best\n",
    "    best_loss, best_loss_ind = findmin(mle_loss)\n",
    "\n",
    "    return inits, inits_loss, mle_vals, mle_loss, best_loss_ind, converged\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a6ddd74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gen_all_fish_data"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    function compute_fi_am(eval_pt::Vector{Float64}, hprm::Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "compute the Fisher information at evaluation point. The Fisher information is given by the trace of the negative Hessian of the log-likelihood function.\n",
    "\n",
    "# Returns\n",
    "- `Float64`: Fisher information value at given evaluation point\n",
    "\"\"\"\n",
    "function compute_fi_am(eval_pt::Vector{Float64}, hprm::Src.Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "    H = ForwardDiff.hessian(x -> compute_ll_am(x, hprm, true_val; t_fixed=t_fixed, t_end=t_end, t_step=t_step), eval_pt)\n",
    "    return tr(-H)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    function store_fish_data(w0::Float64,m::Float64,M::Int64,noise::Float64,df::DataFrame, path::String)\n",
    "\n",
    "stores data evaluated on grid in a csv file.\n",
    "Name of form \"fish_w0_n0_a_m_M_noise.csv\"\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: df to store\n",
    "- `path_to_repo::String`: path to folder where to store the file\n",
    "\"\"\"\n",
    "function store_fish_data(w0::Float64,m::Float64,M::Int64,noise::Float64,df::DataFrame, path::String)\n",
    "    CSV.write(\"$(path)fish_$(w0)_$(m)_$(M)_$(noise).csv\", df)\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    function gen_all_fish_data(M_vals, noise_vals, m, w0, path; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "function that generates and stores all the fish data needed. On all a,n0,M,noise prm combinations specifed.\n",
    "\n",
    "# Arguments\n",
    "- `M_val::Vector{Int64}`: sample sizes\n",
    "- `noise_vals::Vector{Float64}`: noise levels\n",
    "- `m::Float64`: mortality rate in Klausmeier model (fixed)\n",
    "- `w0::Float64`: initial value for water compartment in Klausmeier model (fixed)\n",
    "- `path::String`: path to folder where fish data is stored\n",
    "- `t_fixed::Bool`: true if we consider a fixed observation time window\n",
    "- `t_end::Float64`: end of observation window (if t_fixed=true)\n",
    "- `t_step::Float64`: step size with which M observations should be picked (set if t_fixed=false)\n",
    "\"\"\"\n",
    "function gen_all_fish_data(M_vals, noise_vals, m, w0, path; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "    for M in M_vals\n",
    "        for noise in noise_vals\n",
    "\n",
    "            grid = Src.create_grid()\n",
    "            fish = zeros(41, 21)\n",
    "\n",
    "            # keep track of whether the optimization algo terminates successfully when finding the MLE\n",
    "            success_counter = 0\n",
    "            eval_pt_counter = 0\n",
    "\n",
    "            # evaluate fisher info on grid\n",
    "            for i in range(1, 41)\n",
    "                for j in range(1, 21)\n",
    "                    eval_pt_counter = eval_pt_counter + 1 # total number of optimizations\n",
    "\n",
    "                    pt = grid[i,j] # true observation parameter point\n",
    "                    hprm = Src.Hyperprm(w0, pt[2], pt[1], m, M, noise) # w0,n0,a,m,M\n",
    "\n",
    "                    sol_true = Src.sol_klausmeier(hprm; t_fixed=t_fixed, t_end=t_end, t_step=t_step)\n",
    "                    sol_true = Src.randomize_data!(sol_true, hprm.noise) # include noise\n",
    "\n",
    "                    mle, success = compute_mle_am(hprm, sol_true; t_fixed=t_fixed, t_end=t_end, t_step=t_step)\n",
    "\n",
    "                    # evaluate Fi at MLE\n",
    "                    fish[i,j] = compute_fi_am(mle, hprm, sol_true; t_fixed=t_fixed, t_end=t_end, t_step=t_step)\n",
    "\n",
    "                    success_counter = success_counter + success # number of successfull optimizations\n",
    "                end\n",
    "            end\n",
    "\n",
    "            success_fraction = success_counter / eval_pt_counter\n",
    "            println(\"MLE terminated with success in $success_fraction cases.\")\n",
    "            \n",
    "            # create data frame\n",
    "            a_eval_pts = string.(0.0:0.1:2.0)\n",
    "            df_fish = DataFrame(fish, a_eval_pts)\n",
    "\n",
    "            store_fish_data(w0, m, M, noise, df_fish, path)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258fea38",
   "metadata": {},
   "source": [
    "## infer a,m with fisher inversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99415d77",
   "metadata": {},
   "source": [
    "we infer a,m for different combinations of observed (a,m,n0) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f023215f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general setup\n",
    "t_fixed = true\n",
    "t_end = 100.0;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa2996d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 1.0 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: At t=92.02535082865948, dt was forced below floating point epsilon 1.4210854715202004e-14, and step error estimate = NaN. Aborting. There is either an error in your model specification or the true solution is unstable (or the true solution can not be represented in the precision of Float64).\n",
      "└ @ SciMLBase C:\\Users\\lisah\\.julia\\packages\\SciMLBase\\9sEkh\\src\\integrator_interface.jl:657\n",
      "┌ Warning: At t=88.59461778770601, dt was forced below floating point epsilon 1.4210854715202004e-14, and step error estimate = NaN. Aborting. There is either an error in your model specification or the true solution is unstable (or the true solution can not be represented in the precision of Float64).\n",
      "└ @ SciMLBase C:\\Users\\lisah\\.julia\\packages\\SciMLBase\\9sEkh\\src\\integrator_interface.jl:657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 0.9988385598141696 cases.\n",
      "MLE terminated with success in 0.9976771196283392 cases.\n",
      "MLE terminated with success in 0.9941927990708479 cases.\n",
      "MLE terminated with success in 1.0 cases.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: At t=90.30462876484866, dt was forced below floating point epsilon 1.4210854715202004e-14, and step error estimate = NaN. Aborting. There is either an error in your model specification or the true solution is unstable (or the true solution can not be represented in the precision of Float64).\n",
      "└ @ SciMLBase C:\\Users\\lisah\\.julia\\packages\\SciMLBase\\9sEkh\\src\\integrator_interface.jl:657\n",
      "┌ Warning: At t=90.28057470540992, dt was forced below floating point epsilon 1.4210854715202004e-14, and step error estimate = NaN. Aborting. There is either an error in your model specification or the true solution is unstable (or the true solution can not be represented in the precision of Float64).\n",
      "└ @ SciMLBase C:\\Users\\lisah\\.julia\\packages\\SciMLBase\\9sEkh\\src\\integrator_interface.jl:657\n",
      "┌ Warning: At t=47.40788051001408, dt was forced below floating point epsilon 7.105427357601002e-15, and step error estimate = NaN. Aborting. There is either an error in your model specification or the true solution is unstable (or the true solution can not be represented in the precision of Float64).\n",
      "└ @ SciMLBase C:\\Users\\lisah\\.julia\\packages\\SciMLBase\\9sEkh\\src\\integrator_interface.jl:657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 0.9988385598141696 cases.\n",
      "MLE terminated with success in 0.9953542392566783 cases.\n",
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 0.9941927990708479 cases.\n",
      "MLE terminated with success in 0.9953542392566783 cases.\n",
      "MLE terminated with success in 0.9988385598141696 cases.\n",
      "MLE terminated with success in 0.9988385598141696 cases.\n",
      "MLE terminated with success in 0.9976771196283392 cases.\n",
      "MLE terminated with success in 1.0 cases.\n",
      "MLE terminated with success in 0.9953542392566783 cases.\n",
      "MLE terminated with success in 0.991869918699187 cases.\n",
      "MLE terminated with success in 0.9976771196283392 cases.\n",
      "MLE terminated with success in 0.9976771196283392 cases.\n",
      "MLE terminated with success in 0.9872241579558653 cases.\n"
     ]
    }
   ],
   "source": [
    "path = \"C:/Users/lisah/Documents/Repos/ma-code/data/t_fixed/t100/fisher_am/\"\n",
    "noise_vals = [0.0, 0.01, 0.1, 0.5,  1.0, 2.0]\n",
    "M_vals = [50,100,500,1000]\n",
    "w0 = 0.95\n",
    "m = 0.45 # m of data observations is always 0.45\n",
    "\n",
    "gen_all_fish_data(M_vals, noise_vals, m, w0, path, t_fixed=t_fixed, t_end=t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6c6a8f",
   "metadata": {},
   "source": [
    "## practical identifiability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43a13ad",
   "metadata": {},
   "source": [
    "Plots are analyzed in \"notebooks\\playground\\infer_am\\plots_inverse_uq_am.ipynb\".\n",
    "\n",
    "We analyze the identifiability of the following points:\n",
    "1. a = 0.3, m = 0.45, n0 = 1.5\n",
    "2. a = 0.9, m = 0.45, n0 = 1.5\n",
    "3. a = 1.5, m = 0.45, n0 = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c380ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global parameters\n",
    "m_true = 0.45\n",
    "n0_true = 1.5\n",
    "w0 = 0.95\n",
    "M = 100\n",
    "noise = 0.1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_fixed = true\n",
    "t_end = 50.0;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28019efa",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f771c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_mult_restart_mles(inits::Matrix, mles::Matrix, ind_best::Int64; compare::Bool=true) # TODO: hier fehlt N\n",
    "    plot(mles[:,1], label=\"MLEs\", title=\"Multiple restart MLE of a\",color=:orange, ylabel=\"parameter value\", xlabel=\"restart index\")\n",
    "    hline!([hprm.a], linestyle=:dash, linewidth=2, color=:black,  label=\"true parameter\")\n",
    "    scatter!(1:N, mles[:,1], markershape=:square, markersize=2, color=:orange, label=\"\")\n",
    "\n",
    "    if compare\n",
    "        plot!(inits[:,1], label=\"inits\", color=:blue)\n",
    "        scatter!(1:N, inits[:,1], markershape=:square, markersize=2, color=:blue, label=\"\")\n",
    "    end\n",
    "    scatter!([best_loss_ind], [mles[best_loss_ind, 1]], markershape=:x, markerstrokewidth=4, markersize=6, color=:red, label=\"best estimate\")\n",
    "end\n",
    "\n",
    "function plot_mult_restart_losses(inits_loss::Vector, losses::Vector, ind_best::Int64; compare::Bool=true) # TODO: hier fehlt N\n",
    "    plot(losses, label=\"MLEs\", color=:darkorange, title=\"Loss evolution of multiple restart MLE\", ylabel = \"loss value\", xlabel=\"restart index\")\n",
    "    scatter!(1:N, losses, markershape=:square, markersize=2, color=:darkorange, label=\"\")\n",
    "\n",
    "    if compare\n",
    "        plot!(inits_loss, label=\"init\", color=:blue)\n",
    "        scatter!(1:N, inits_loss, markershape=:square, markersize=2, color=:blue, label=\"\")\n",
    "    end\n",
    "\n",
    "    scatter!([best_loss_ind], [losses[best_loss_ind]], markershape=:x, markerstrokewidth=4, markersize=6, color=:red, label=\"lowest\")\n",
    "end\n",
    "\n",
    "function correlation_matrix(eval_pt::Vector{Float64}, hprm::Src.Hyperprm, true_val::DataFrame; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "    fim = - ForwardDiff.hessian(x -> Src.compute_ll(x, hprm, true_val; t_fixed=t_fixed, t_end=t_end, t_step=t_step), eval_pt)\n",
    "    cov = inv(fim)\n",
    "    cor = [cov[i,j] / sqrt(cov[i,i]*cov[j,j]) for i in range(1, size(cov,1)), j in range(1, size(cov,2))]\n",
    "    return cor\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf020399",
   "metadata": {},
   "source": [
    "### 1. Left of bifurcation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e56a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 0.3\n",
    "hprm = Src.Hyperprm(w0, n0_true, a_true, m_true, M, noise)\n",
    "\n",
    "# generate true data, i.e. data observations\n",
    "sol_true = Src.sol_klausmeier(hprm; t_fixed=t_fixed, t_end=t_end) \n",
    "sol_true = Src.randomize_data!(sol_true, hprm.noise); # make data noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1a40c",
   "metadata": {},
   "source": [
    "#### Multiple restart optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf14b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "inits, inits_loss, mles, losses, best_loss_ind, converged = mult_restart_mle_am(N, hprm, sol_true, t_fixed=t_fixed, t_end=t_end);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot inits vs. mles\n",
    "plot_mult_restart_mles(inits, mles, best_loss_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cd0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_mult_restart_losses(inits_loss, losses, best_loss_ind, compare=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8740761",
   "metadata": {},
   "source": [
    "#### Correlation matrix of MLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24b92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_pt = mles[best_loss_ind,:]\n",
    "correlation_matrix(mle_pt, hprm, sol_true, t_fixed=t_fixed, t_end=t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e09eaf",
   "metadata": {},
   "source": [
    "### 2. At bifurcation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca9526",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 0.9\n",
    "hprm = Src.Hyperprm(w0, n0_true, a_true, m_true, M, noise)\n",
    "\n",
    "# generate true data, i.e. data observations\n",
    "sol_true = Src.sol_klausmeier(hprm; t_fixed=t_fixed, t_end=t_end) \n",
    "sol_true = Src.randomize_data!(sol_true, hprm.noise); # make data noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e2079",
   "metadata": {},
   "source": [
    "#### Multiple restart optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8b52ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "inits, inits_loss, mles, losses, best_loss_ind, converged = mult_restart_mle_am(N, hprm, sol_true, t_fixed=t_fixed, t_end=t_end);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23ec917",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7265f7fc",
   "metadata": {},
   "source": [
    "#### Correlation matrix of MLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad7eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_pt = mles[best_loss_ind,:]\n",
    "correlation_matrix(mle_pt, hprm, sol_true, t_fixed=t_fixed, t_end=t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c2141",
   "metadata": {},
   "source": [
    "### 3. Right of bifurcation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896b9802",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_true = 1.5\n",
    "hprm = Src.Hyperprm(w0, n0_true, a_true, m_true, M, noise)\n",
    "\n",
    "# generate true data, i.e. data observations\n",
    "sol_true = Src.sol_klausmeier(hprm; t_fixed=t_fixed, t_end=t_end) \n",
    "sol_true = Src.randomize_data!(sol_true, hprm.noise); # make data noisy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53631b9a",
   "metadata": {},
   "source": [
    "#### Multiple restart optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a055f7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "inits, inits_loss, mles, losses, best_loss_ind, converged = mult_restart_mle_am(N, hprm, sol_true, t_fixed=t_fixed, t_end=t_end);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967eb2a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc32b673",
   "metadata": {},
   "source": [
    "#### Correlation matrix of MLEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70311a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_pt = mles[best_loss_ind,:]\n",
    "correlation_matrix(mle_pt, hprm, sol_true, t_fixed=t_fixed, t_end=t_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1698da98",
   "metadata": {},
   "source": [
    "## generate likelihood data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    function gen_ll_evals_for_hprm_comb(hprm_true::Hyperprm; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "evaluates log-likelihood on grid for one (a,n0,M,noise) hyperprm combination. Run this for all hyperprm combinations wanted, helper function\n",
    "\n",
    "# Arguments\n",
    "- `hprm::Hyperprm`: parameters for which the Klausmeier simulation is performed\n",
    "- `true_val::DataFrame`: true data trajectories. DataFrame with columns \"w\" and \"n\".\n",
    "- `t_fixed::Bool`: true if we consider a fixed observation time window\n",
    "- `t_end::Float64`: end of observation window (if t_fixed=true)\n",
    "- `t_step::Float64`: step size with which M observations should be picked (set if t_fixed=false)\n",
    "\n",
    "# Returns\n",
    "-`DataFrame`: DataFrame of log-likelihood evaluated on grid for given parameter combination\n",
    "\"\"\"\n",
    "function gen_ll_evals_for_am_comb(hprm_true::Src.Hyperprm; t_fixed::Bool=false, t_end::Float64=50.0, t_step::Float64=1.0)\n",
    "\n",
    "    grid = Src.create_grid()\n",
    "    sol_true = Src.sol_klausmeier(hprm_true; t_fixed=t_fixed, t_end=t_end, t_step=t_step) # returns df\n",
    "    sol_true = Src.randomize_data!(sol_true, hprm_true.noise) # include noise\n",
    "\n",
    "    ll = zeros(11, 21) # change grid size, because not necessary to vary m this much\n",
    "\n",
    "    for i in range(1, 11)\n",
    "        for j in range(1, 21) #eval for each point on grid\n",
    "            pt = grid[i,j]\n",
    "            ll[i,j] = compute_ll_am([pt[1],pt[2]], hprm_true, sol_true; t_fixed=t_fixed, t_end=t_end, t_step=t_step)\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    #return data frame\n",
    "    a_eval_pts = string.(0.0:0.1:2.0)\n",
    "    df_ll = DataFrame(ll, a_eval_pts)\n",
    "\n",
    "    return df_ll\n",
    "end\n",
    "\n",
    "\"\"\"\n",
    "    function store_ll_data(w0::Float64,n0::Float64,a::Float64,m::Float64,M::Int64,noise::Float64,df::DataFrame, path_to_repo)\n",
    "\n",
    "stores data evaluated on grid in a csv file.\n",
    "Name of form \"ll_w0_n0_a_m_M_noise.csv\"\n",
    "\n",
    "# Arguments\n",
    "- `df::DataFrame`: df to store\n",
    "- `path_to_repo::String`: path to folder where to store the file\n",
    "\"\"\"\n",
    "function store_ll_data(w0::Float64,n0::Float64,a::Float64,m::Float64,M::Int64,noise::Float64,df::DataFrame, path_to_store::String)\n",
    "    CSV.write(\"$(path_to_store)ll_$(w0)_$(n0)_$(a)_$(m)_$(M)_$(noise).csv\", df)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.5",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
